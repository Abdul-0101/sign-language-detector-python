<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Sign-Language Detector</title>
  <style>
    /* ... your existing styles ... */
    video {
      width: 480px;
      height: 360px;
      transform: scaleX(-1); /* mirror */
      border: 4px solid #333;
    }
  </style>
</head>
<body>
  <!-- ... heading, output div, inputs/buttons ... -->

  <!-- 1) Browser webcam capture -->
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas" width="480" height="360" style="display:none"></canvas>

  <!-- ... your input and buttons ... -->

  <script src="{{ url_for('static', filename='inference_classifier.js') }}"></script>
  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');

    // Start the webcam
    navigator.mediaDevices.getUserMedia({ video: true })
      .then(stream => { video.srcObject = stream; })
      .catch(err => { console.error('Webcam error:', err); });

    // Main loop: grab frame, send to /predict, update UI
    function loop() {
      if (video.readyState === video.HAVE_ENOUGH_DATA) {
        // Draw mirrored frame to canvas
        ctx.save();
        ctx.scale(-1, 1);
        ctx.drawImage(video, -canvas.width, 0, canvas.width, canvas.height);
        ctx.restore();

        const dataURL = canvas.toDataURL('image/jpeg');
        inference_classifier.predict(dataURL).then(label => {
          outputEl.textContent = `Detected: ${label}`;
          updateSuggestion(label);
        });
      }
      requestAnimationFrame(loop);
    }

    video.addEventListener('loadeddata', () => {
      loop();
    });

    // ... rest of your existing JS (autocomplete, buttons, keys) ...
  </script>
</body>
</html>
